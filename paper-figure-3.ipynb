{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e482981",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradient_estimation_naive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2061039/1122507482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearn_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magent_distribution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_continuity_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_point_interpolation_true_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_unit_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptimal_beta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimal_beta_expected_policy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreparametrized_gradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_total_derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/ir.stanford.edu/users/r/s/rsahoo/public/policy-learning-competing-agents/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magent_distribution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradient_estimation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradient_estimation_naive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientEstimatorNaive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexpected_gradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExpectedGradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexpected_gradient_naive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExpectedGradientNaive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradient_estimation_naive'"
     ]
    }
   ],
   "source": [
    "from train import learn_model\n",
    "from agent_distribution import AgentDistribution\n",
    "from utils import compute_continuity_noise, fixed_point_interpolation_true_distribution, convert_to_unit_vector\n",
    "from optimal_beta import optimal_beta_expected_policy_loss \n",
    "from reparametrized_gradient import plot_total_derivative\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6591e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_challenging_agent_dist(n, n_types, d):\n",
    "    gaming_type_etas = np.random.uniform(3., 5., int(n_types * d/2)).reshape(int(n_types/2), d, 1)\n",
    "    gaming_type_gamma_one = np.random.uniform(0.01, 0.02, int(n_types/2) ).reshape(int(n_types/2), 1, 1)\n",
    "    gaming_type_gamma_two = np.random.uniform(10., 20., int(n_types/2)).reshape(int(n_types/2), 1, 1)\n",
    "    gaming_type_gammas = np.hstack((gaming_type_gamma_one, gaming_type_gamma_two))\n",
    "    \n",
    "    natural_type_etas = np.random.uniform(5., 7., int(n_types *d/2)).reshape(int(n_types/2), d, 1)\n",
    "    natural_type_gammas = np.random.uniform(10., 20., int(n_types * d/2)).reshape(int(n_types/2), d, 1)\n",
    "    \n",
    "    etas = np.vstack((gaming_type_etas, natural_type_etas))\n",
    "    gammas = np.vstack((gaming_type_gammas, natural_type_gammas))\n",
    "    dic = {\"etas\": etas, \"gammas\": gammas}\n",
    "    agent_dist = AgentDistribution(n=n, d=d, n_types=n_types, types=dic, prop=None)\n",
    "    return agent_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab89502",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "n_types = 10\n",
    "d = 2\n",
    "np.random.seed(0)\n",
    "agent_dist = create_challenging_agent_dist(n, n_types, d)\n",
    "sigma = compute_continuity_noise(agent_dist)\n",
    "q = 0.7\n",
    "f = fixed_point_interpolation_true_distribution(agent_dist, sigma, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ea00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss, opt_beta, opt_s_beta, thetas, losses = optimal_beta_expected_policy_loss(agent_dist, sigma, f, plot=False)\n",
    "opt_theta = np.arctan2(opt_beta[1], opt_beta[0])\n",
    "print(\"Opt Theta: {}\".format(opt_theta.item()))\n",
    "print(\"Min Loss: {}\".format(min_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "thetas = np.linspace(0, 2*np.pi, len(losses))\n",
    "ax[0].plot(thetas, -np.array(losses), c=\"black\")\n",
    "ax[0].scatter([0.], [1.48], label=r\"$\\beta = \\tilde{\\beta}$\" + \" \" + r\"$(\\theta= 0)$\", c=\"r\", s=50, alpha=1)\n",
    "ax[0].scatter([1.218], [1.616], label=r\"$\\beta = \\beta^{*}$\" + \" \" + r\"$(\\theta= 1.22)$\", c=\"g\", s=50, alpha=1)\n",
    "ax[0].set_xlabel(r\"$\\theta$\", fontsize=15)\n",
    "ax[0].set_ylabel(\"Equilibrium Policy Value\", fontsize=15)\n",
    "ax[0].set_title(\"Equilibrium Policy Value\" + \" vs. \" + r\"$\\theta$\", fontsize=15)\n",
    "ax[0].legend(fontsize=12)\n",
    "\n",
    "all_br = agent_dist.best_response_noisy_score_distribution(opt_beta, opt_s_beta, sigma)\n",
    "natural_types_idx = np.where(agent_dist.n_agent_types >= int(n_types/2))\n",
    "gaming_types_idx = np.where(agent_dist.n_agent_types < int(n_types/2))\n",
    "\n",
    "ax[2].hist(all_br[natural_types_idx], label=\"Naturals\", alpha=0.5)\n",
    "ax[2].hist(all_br[gaming_types_idx], label=\"Gamers\", alpha=0.5)\n",
    "ax[2].axvline(x=opt_s_beta, c=\"purple\", label=r\"$s(\\beta^{*})$\")\n",
    "ax[2].set_title(\"Score Distribution when \" + r\"$\\beta = \\beta^{*}$\", fontsize=15)\n",
    "ax[2].set_xlabel(\"Scores\", fontsize=15)\n",
    "ax[2].set_ylabel(\"Count\", fontsize=15)\n",
    "ax[2].legend(fontsize=12)\n",
    "\n",
    "nat_above = all_br[natural_types_idx] > opt_s_beta\n",
    "gam_above = all_br[gaming_types_idx] > opt_s_beta\n",
    "\n",
    "print(nat_above.sum()/ (nat_above.sum() + gam_above.sum()))\n",
    "\n",
    "default_theta = 0.\n",
    "default_beta = convert_to_unit_vector(np.array([default_theta]).reshape(1,1))\n",
    "default_s_beta = f(default_theta)\n",
    "\n",
    "all_br = agent_dist.best_response_noisy_score_distribution(default_beta, default_s_beta, sigma)\n",
    "natural_types_idx = np.where(agent_dist.n_agent_types >= int(n_types/2))\n",
    "gaming_types_idx = np.where(agent_dist.n_agent_types < int(n_types/2))\n",
    "\n",
    "nat_above = all_br[natural_types_idx] > default_s_beta\n",
    "gam_above = all_br[gaming_types_idx] > default_s_beta\n",
    "\n",
    "print(nat_above.sum()/ (nat_above.sum() + gam_above.sum()))\n",
    "\n",
    "ax[1].hist(all_br[natural_types_idx], label=\"Naturals\", alpha=0.5)\n",
    "ax[1].hist(all_br[gaming_types_idx], label=\"Gamers\", alpha=0.5)\n",
    "ax[1].axvline(x=default_s_beta, c=\"purple\", label=r\"$s(\\tilde{\\beta})$\")\n",
    "ax[1].set_title(\"Score Distribution when \" + r\"$\\beta = \\tilde{\\beta}$\", fontsize=15)\n",
    "ax[1].set_xlabel(\"Scores\", fontsize=15)\n",
    "ax[1].set_ylabel(\"Count\", fontsize=15)\n",
    "ax[1].legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figs/paper-figure-3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bf332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
